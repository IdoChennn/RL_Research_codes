{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97415405",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2d2e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "statistical_heterogeneity = 5\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)  # Change from 1 to 3 input channels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)  # Adjust the size for CIFAR-10\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)  # Adjust the size for CIFAR-10\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def create_non_iid_partitions(dataset, num_clients):\n",
    "    num_classes = 10\n",
    "    class_indices = [[] for _ in range(num_classes)]\n",
    "    \n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Ensure randomness in class selection for each client\n",
    "    all_classes = list(range(num_classes))\n",
    "    \n",
    "    client_local_datasets = []\n",
    "    for i in range(num_clients):\n",
    "        # Randomly select two major classes for each client\n",
    "        major_classes = random.sample(all_classes, 2)\n",
    "\n",
    "        # Allocate all data from the two major classes\n",
    "        client_indices = class_indices[major_classes[0]] + class_indices[major_classes[1]]\n",
    "        \n",
    "        #Add a small number of samples from other classes\n",
    "        minor_indices = []\n",
    "        for cls in set(range(num_classes)) - set(major_classes):\n",
    "            n_samples = len(class_indices[cls]) // num_clients // statistical_heterogeneity  # 50 times less than major classes\n",
    "            minor_indices.extend(class_indices[cls][i * n_samples: (i + 1) * n_samples])\n",
    "\n",
    "        client_indices = client_indices + minor_indices\n",
    "        random.shuffle(client_indices)  # Shuffle to mix data from different classes\n",
    "        client_local_datasets.append(data_utils.Subset(dataset, client_indices))\n",
    "\n",
    "    return client_local_datasets\n",
    "\n",
    "def create_iid_partitions(dataset, num_clients):\n",
    "\n",
    "    client_datasets = []\n",
    "    for i in range(num_clients):\n",
    "        client_dataset = data_utils.Subset(dataset, list(range(i * len(dataset) // num_clients, (i + 1) * len(dataset) // num_clients)))\n",
    "        client_datasets.append(client_dataset)\n",
    "    return client_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1c226",
   "metadata": {},
   "source": [
    "# Training set up using CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6e1fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "myGPU = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(myGPU)\n",
    "# load the data\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB channels\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "global_epochs = 20\n",
    "local_epochs = 10\n",
    "number_of_clients = 5\n",
    "client_datasets=create_iid_partitions(trainset, number_of_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e8c1f",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbdd224146c7e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:29:55.950535092Z",
     "start_time": "2024-03-25T20:29:55.940586425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def server_aggregate(model, state_dict_list):\n",
    "    # average the model\n",
    "    aggregated_state = {}\n",
    "    for key,parameter in model.named_parameters():\n",
    "        tensor_to_aggregate = []\n",
    "        for client_state_dict in state_dict_list:\n",
    "            client_tensor = client_state_dict[key].float()\n",
    "            tensor_to_aggregate.append(client_tensor)\n",
    "        \n",
    "        stacked_tensor = torch.stack(tensor_to_aggregate,dim=0)\n",
    "        mean_tensor = torch.mean(stacked_tensor,dim=0)\n",
    "        aggregated_state[key] = mean_tensor\n",
    "    model.load_state_dict(aggregated_state)\n",
    "\n",
    "    return model \n",
    "\n",
    "def difference_models_norm_2(local_model, initial_model):\n",
    "    tensor_1 = list(local_model.parameters())\n",
    "    tensor_2 = list(initial_model.parameters())\n",
    "    sub_norm = []\n",
    "    for i in range(len(tensor_1)):\n",
    "        s = torch.norm(tensor_1[i].to(myGPU) - tensor_2[i].to(myGPU),p=2)\n",
    "        sub_norm.append(s)\n",
    "    return sum(sub_norm)\n",
    "\n",
    "def client_update(received_model, train_data, local_optimizer, loss_f, epoch,client_id,mu,algorithm,sys_heter):\n",
    "    local_model = received_model.to(myGPU)\n",
    "    initial_model = received_model.to(myGPU)\n",
    "\n",
    "    random_chance = random.randint(0, 10) # Randomly decide if the client is weak or strong\n",
    "    if random_chance >= sys_heter or algorithm == \"FedProx\":\n",
    "    \n",
    "        print(f\"Client {client_id+1} starts training...\")\n",
    "        local_model.train()\n",
    "        \n",
    "        for i in range(epoch):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for feature, label in train_data:\n",
    "                local_optimizer.zero_grad()\n",
    "                feature, label = feature.to(myGPU), label.to(myGPU)\n",
    "                outputs = local_model(feature)\n",
    "                local_loss = loss_f(outputs, label)\n",
    "                loss_prox = (mu / 2) * difference_models_norm_2(local_model, initial_model) # perform model updates penalization using proximal term\n",
    "                loss = local_loss + loss_prox\n",
    "                loss.backward()\n",
    "                local_optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "      \n",
    "            print(f\"Epoch {i+1} loss: {running_loss / len(train_data)}\")\n",
    "        print(\"\\n\")\n",
    "        return local_model.state_dict()\n",
    "        # return model\n",
    "    \n",
    "    else: # If the client is weak, it will not train\n",
    "        print(f\"Client {client_id+1} dropped\")\n",
    "        return local_model.state_dict()\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_labels in testloader:\n",
    "         \n",
    "            test_feature, test_labels = test_feature.to(myGPU), test_labels.to(myGPU)\n",
    "            outputs = model(test_feature)\n",
    "            test_loss = loss_function(outputs, test_labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += test_labels.size(0)\n",
    "            correct += (predicted == test_labels).sum().item()\n",
    "            test_loss += test_loss.item()\n",
    "    accuracy = 100 * correct / total\n",
    "    loss = test_loss / len(testloader)\n",
    "    loss = loss.item()\n",
    "\n",
    "    # print(f\"Accuracy of the network: {accuracy}, Loss: {loss}\")\n",
    "    return [accuracy, loss]\n",
    "\n",
    "def federated_learning(model, mu,client_datasets, testloader, optimizer, loss_function, global_epochs, local_epochs,algorithm,sys_heter):\n",
    "    \n",
    "    for i, client_dataset in enumerate(client_datasets):\n",
    "        client_labels = [trainset.targets[idx] for idx in client_dataset.indices]  # Access the labels for each subset\n",
    "        unique_labels = np.unique(client_labels)\n",
    "        # print(f\"Client {i} has {len(unique_labels)} unique labels: {unique_labels}, and {len(client_dataset)} samples\")\n",
    "        \n",
    "    # initial_phase = evaluate(model, testloader)     \n",
    "    \n",
    "    # Create a dataloader for each client\n",
    "    client_dataloaders = [data_utils.DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2) for dataset in client_datasets]\n",
    "    global_history = []\n",
    "    global_model = model.to(myGPU)\n",
    "    client_model_lists = []\n",
    "    client_optimizer_lists = []\n",
    "    for k in range(number_of_clients):\n",
    "        client_model_lists.append(deepcopy(global_model))\n",
    "        client_optimizer_lists.append(optim.SGD(client_model_lists[k].parameters(), lr=0.01, momentum=0.9))\n",
    "\n",
    "    for global_epoch in range(global_epochs):\n",
    "        \n",
    "        print(f\"---- Global Round {global_epoch+1}: ----\") \n",
    "        # print(f\"'{'Global Round':=^100}'\")\n",
    "        state_dicts = []\n",
    "      \n",
    "        for client_id, client_dataloader in enumerate(client_dataloaders):\n",
    "            client_state_dict = client_update(client_model_lists[client_id], client_dataloader, client_optimizer_lists[client_id], loss_function, local_epochs,client_id,mu,algorithm,sys_heter)\n",
    "            state_dicts.append(client_state_dict)\n",
    "        \n",
    "        global_model = server_aggregate(global_model, state_dicts)\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        client_model_lists.clear\n",
    "        for k in range(number_of_clients):\n",
    "            client_model_lists.append(deepcopy(model))\n",
    "        \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9e535955a1ff9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reinforcement Learning Environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29f927e01381ba10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T20:37:57.684442207Z",
     "start_time": "2024-03-25T20:37:57.611694618Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class FedProxTuningEnv(gym.Env):\n",
    "    def __init__(self, fl_model,train_datasets, test_loader, optimizer, loss_function, global_epochs=2, local_epochs=5, algorithm='FedProx', sys_heter=5):\n",
    "        super(FedProxTuningEnv, self).__init__()\n",
    "\n",
    "        self.fl_model = fl_model\n",
    "        self.train_datasets = train_datasets\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.global_epochs = global_epochs\n",
    "        self.local_epochs = local_epochs\n",
    "        self.algorithm = algorithm\n",
    "        self.sys_heter = sys_heter\n",
    "\n",
    "        self.current_mu = 0.1  # Initial mu value\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(2)  # Actions: 0 for decreasing mu, 1 for increasing mu\n",
    "        self.observation_space = spaces.Box(low=np.array([0.0, 0]), high=np.array([10.0, global_epochs]), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        # Use the federated learning model for evaluation\n",
    "        print(f\"\\n'{'Restarting Environment':=^100}'\\n\")\n",
    "        initial_metrics = evaluate(self.fl_model, self.test_loader)  # Make sure this is the CNN model\n",
    "        print(\"Current accuracy: \", initial_metrics[0], \"Current loss: \", initial_metrics[1])\n",
    "        print(\"Current mu: \", self.current_mu)\n",
    "        initial_loss = initial_metrics[1].item() if torch.is_tensor(initial_metrics[1]) else initial_metrics[1]\n",
    "        self.fl_model = CNN().to(myGPU)\n",
    "        self.optimizer = optim.SGD(self.fl_model.parameters(), lr=0.01, momentum=0.9)\n",
    "        return np.array([self.current_mu, initial_loss], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        print(f\"\\n'{'Stepping':=^50}'\\n\")\n",
    "        # Adjust mu based on action\n",
    "        print(f'Action take: {action}')\n",
    "       \n",
    "        if action == 0 and self.current_mu >= 0.02:  # Ensure mu stays positive\n",
    "            print(f'Decreasing mu to {self.current_mu - 0.01}')\n",
    "            self.current_mu -= 0.01\n",
    "        elif action == 1 and self.current_mu < 10.0:  # Upper bound for mu\n",
    "            print(f'Increasing mu to {self.current_mu + 0.01}')\n",
    "            self.current_mu += 0.01\n",
    "        \n",
    "        previous_metrics = evaluate(self.fl_model, self.test_loader)\n",
    "        # Run one global epoch of federated learning with the current mu\n",
    "        # print(f'Running global epoch {self.current_round + 1} with mu = {self.current_mu}')\n",
    "        print(f\"\\n'{'Start Federated Learning':-^50}'\\n\")\n",
    "        global_model = federated_learning(self.fl_model, self.current_mu, self.train_datasets, self.test_loader, self.optimizer, self.loss_function, self.global_epochs, self.local_epochs, self.algorithm, self.sys_heter)\n",
    "        \n",
    "        current_metrics = evaluate(global_model, self.test_loader)\n",
    "        \n",
    "        # Update the state\n",
    "\n",
    "        state = np.array([self.current_mu, current_metrics[1]])  # Using loss as part of the state\n",
    "        # Reward is based on improvement in accuracy\n",
    "        reward = current_metrics[0] - previous_metrics[0]  # Change in accuracy\n",
    "        print(f'Previous Accuracy: {previous_metrics[0]}')\n",
    "        print(f'Previous Loss: {previous_metrics[1]}')\n",
    "        print(f'Current Accuracy: {current_metrics[0]}')\n",
    "        print(f'Current Loss: {current_metrics[1]}')\n",
    "        print(f'New state: {state}')\n",
    "        print(f'Reward: {reward}')\n",
    "\n",
    "        print(f\"\\n'{'End Federated Learning':-^50}'\\n\")\n",
    "        if abs(current_metrics[1] - previous_metrics[1]) <= 0.01:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "    \n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f'Round: {self.current_round}, Mu: {self.current_mu}')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f95dc7d5777d3e9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-25T20:37:58.965128149Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  10.0 Current loss:  0.11544054001569748\n",
      "Current mu:  0.1\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.300601178407669\n",
      "Epoch 2 loss: 2.2837338984012603\n",
      "Epoch 3 loss: 2.1770746231079103\n",
      "Epoch 4 loss: 1.9653030395507813\n",
      "Epoch 5 loss: 1.8794841468334198\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3009760677814484\n",
      "Epoch 2 loss: 2.2853176474571226\n",
      "Epoch 3 loss: 2.192707124352455\n",
      "Epoch 4 loss: 1.999227985739708\n",
      "Epoch 5 loss: 1.9011014938354491\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.300685441493988\n",
      "Epoch 2 loss: 2.2831737816333773\n",
      "Epoch 3 loss: 2.174362841248512\n",
      "Epoch 4 loss: 1.9755433917045593\n",
      "Epoch 5 loss: 1.8740404844284058\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.300489765405655\n",
      "Epoch 2 loss: 2.2860056161880493\n",
      "Epoch 3 loss: 2.201464146375656\n",
      "Epoch 4 loss: 1.9964824974536897\n",
      "Epoch 5 loss: 1.9295986473560334\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.300785058736801\n",
      "Epoch 2 loss: 2.284102565050125\n",
      "Epoch 3 loss: 2.187352639436722\n",
      "Epoch 4 loss: 1.9888497799634934\n",
      "Epoch 5 loss: 1.8910364627838134\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.797917503118515\n",
      "Epoch 2 loss: 1.689602106809616\n",
      "Epoch 3 loss: 1.6359143197536468\n",
      "Epoch 4 loss: 1.580823254585266\n",
      "Epoch 5 loss: 1.5438572764396667\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.826458576321602\n",
      "Epoch 2 loss: 1.7567386209964753\n",
      "Epoch 3 loss: 1.7115630596876144\n",
      "Epoch 4 loss: 1.6224600285291673\n",
      "Epoch 5 loss: 1.5886848002672196\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.7936399340629579\n",
      "Epoch 2 loss: 1.733813965320587\n",
      "Epoch 3 loss: 1.6314697235822677\n",
      "Epoch 4 loss: 1.5842577666044235\n",
      "Epoch 5 loss: 1.534248098731041\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.8573298305273056\n",
      "Epoch 2 loss: 1.7424338191747666\n",
      "Epoch 3 loss: 1.6924337238073348\n",
      "Epoch 4 loss: 1.6245597630739212\n",
      "Epoch 5 loss: 1.598943692445755\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.814020222425461\n",
      "Epoch 2 loss: 1.7089693635702132\n",
      "Epoch 3 loss: 1.6432160794734956\n",
      "Epoch 4 loss: 1.6105055272579194\n",
      "Epoch 5 loss: 1.5629783809185027\n",
      "\n",
      "\n",
      "Previous Accuracy: 8.52\n",
      "Previous Loss: 0.1156492754817009\n",
      "Current Accuracy: 44.27\n",
      "Current Loss: 0.07318249344825745\n",
      "New state: [0.09       0.07318249]\n",
      "Reward: 35.75\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.27 Current loss:  0.07318249344825745\n",
      "Current mu:  0.09000000000000001\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.08000000000000002\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.302385687828064\n",
      "Epoch 2 loss: 2.295759379863739\n",
      "Epoch 3 loss: 2.2677611231803896\n",
      "Epoch 4 loss: 2.1360098481178285\n",
      "Epoch 5 loss: 1.9858658134937286\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.302150321006775\n",
      "Epoch 2 loss: 2.295999199151993\n",
      "Epoch 3 loss: 2.2728679835796357\n",
      "Epoch 4 loss: 2.1752397537231447\n",
      "Epoch 5 loss: 2.0379233717918397\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.301603800058365\n",
      "Epoch 2 loss: 2.2949052333831785\n",
      "Epoch 3 loss: 2.269044500589371\n",
      "Epoch 4 loss: 2.1374378710985185\n",
      "Epoch 5 loss: 1.9819209188222886\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.302536702156067\n",
      "Epoch 2 loss: 2.2963568568229675\n",
      "Epoch 3 loss: 2.2766323745250703\n",
      "Epoch 4 loss: 2.17119961977005\n",
      "Epoch 5 loss: 2.0254943430423737\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3022048652172087\n",
      "Epoch 2 loss: 2.2961422204971313\n",
      "Epoch 3 loss: 2.2746235966682433\n",
      "Epoch 4 loss: 2.160952764749527\n",
      "Epoch 5 loss: 2.01285939514637\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8628009557724\n",
      "Epoch 2 loss: 1.7560411155223847\n",
      "Epoch 3 loss: 1.6734815746545793\n",
      "Epoch 4 loss: 1.6361528038978577\n",
      "Epoch 5 loss: 1.5508944034576415\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.92025066614151\n",
      "Epoch 2 loss: 1.8439116567373275\n",
      "Epoch 3 loss: 1.7438766449689864\n",
      "Epoch 4 loss: 1.6613972514867783\n",
      "Epoch 5 loss: 1.5873865157365799\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.858474424481392\n",
      "Epoch 2 loss: 1.7633096575737\n",
      "Epoch 3 loss: 1.681009441614151\n",
      "Epoch 4 loss: 1.6099181830883027\n",
      "Epoch 5 loss: 1.5481403678655625\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.9253731936216354\n",
      "Epoch 2 loss: 1.8121621757745743\n",
      "Epoch 3 loss: 1.7326937347650528\n",
      "Epoch 4 loss: 1.6758756339550018\n",
      "Epoch 5 loss: 1.5953536719083785\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.88077732026577\n",
      "Epoch 2 loss: 1.7678943306207657\n",
      "Epoch 3 loss: 1.6925179034471511\n",
      "Epoch 4 loss: 1.640695610642433\n",
      "Epoch 5 loss: 1.606802186369896\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11420891433954239\n",
      "Current Accuracy: 43.24\n",
      "Current Loss: 0.06978025287389755\n",
      "New state: [0.08       0.06978025]\n",
      "Reward: 33.24\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  43.24 Current loss:  0.06978025287389755\n",
      "Current mu:  0.08000000000000002\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3000733315944673\n",
      "Epoch 2 loss: 2.283483511209488\n",
      "Epoch 3 loss: 2.206752973794937\n",
      "Epoch 4 loss: 2.0606791883707047\n",
      "Epoch 5 loss: 1.9418702632188798\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.300294315814972\n",
      "Epoch 2 loss: 2.2857092022895813\n",
      "Epoch 3 loss: 2.2151007294654845\n",
      "Epoch 4 loss: 2.0800289064645767\n",
      "Epoch 5 loss: 1.9620782017707825\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3010733902454374\n",
      "Epoch 2 loss: 2.2842671275138855\n",
      "Epoch 3 loss: 2.2060124039649964\n",
      "Epoch 4 loss: 2.0589423537254334\n",
      "Epoch 5 loss: 1.9240069955587387\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.3009602427482605\n",
      "Epoch 2 loss: 2.2853671073913575\n",
      "Epoch 3 loss: 2.2107912838459014\n",
      "Epoch 4 loss: 2.0807518780231478\n",
      "Epoch 5 loss: 1.9486065238714219\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3007132172584535\n",
      "Epoch 2 loss: 2.2816171288490295\n",
      "Epoch 3 loss: 2.2007024765014647\n",
      "Epoch 4 loss: 2.0698051303625107\n",
      "Epoch 5 loss: 1.9370053470134736\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8525948345661163\n",
      "Epoch 2 loss: 1.7410620629787446\n",
      "Epoch 3 loss: 1.6403099983930587\n",
      "Epoch 4 loss: 1.601085975766182\n",
      "Epoch 5 loss: 1.5424632996320724\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.8666506588459015\n",
      "Epoch 2 loss: 1.7506817400455474\n",
      "Epoch 3 loss: 1.6840560227632522\n",
      "Epoch 4 loss: 1.6404812067747117\n",
      "Epoch 5 loss: 1.5424515813589097\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8256251960992813\n",
      "Epoch 2 loss: 1.7121828943490982\n",
      "Epoch 3 loss: 1.6264453381299973\n",
      "Epoch 4 loss: 1.5737888425588609\n",
      "Epoch 5 loss: 1.510178017616272\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.849056574702263\n",
      "Epoch 2 loss: 1.7712403386831284\n",
      "Epoch 3 loss: 1.7036419451236724\n",
      "Epoch 4 loss: 1.6276723742485046\n",
      "Epoch 5 loss: 1.5873783588409425\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8630494862794875\n",
      "Epoch 2 loss: 1.7649501323699952\n",
      "Epoch 3 loss: 1.651156297326088\n",
      "Epoch 4 loss: 1.6215784132480622\n",
      "Epoch 5 loss: 1.5508777767419815\n",
      "\n",
      "\n",
      "Previous Accuracy: 11.42\n",
      "Previous Loss: 0.1147928461432457\n",
      "Current Accuracy: 44.84\n",
      "Current Loss: 0.06999858468770981\n",
      "New state: [0.09       0.06999858]\n",
      "Reward: 33.42\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.84 Current loss:  0.06999858468770981\n",
      "Current mu:  0.09000000000000001\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.08000000000000002\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3037765085697175\n",
      "Epoch 2 loss: 2.2908447265625\n",
      "Epoch 3 loss: 2.2161798775196075\n",
      "Epoch 4 loss: 2.0260043293237686\n",
      "Epoch 5 loss: 1.9234118610620499\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.303547924757004\n",
      "Epoch 2 loss: 2.291838216781616\n",
      "Epoch 3 loss: 2.2336630165576934\n",
      "Epoch 4 loss: 2.0464213848114015\n",
      "Epoch 5 loss: 1.9504239022731782\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.302984577417374\n",
      "Epoch 2 loss: 2.290255570411682\n",
      "Epoch 3 loss: 2.213683420419693\n",
      "Epoch 4 loss: 2.0084338754415514\n",
      "Epoch 5 loss: 1.9035911977291107\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.30365914106369\n",
      "Epoch 2 loss: 2.292593240737915\n",
      "Epoch 3 loss: 2.238221913576126\n",
      "Epoch 4 loss: 2.0562692821025848\n",
      "Epoch 5 loss: 1.9421447396278382\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3030362844467165\n",
      "Epoch 2 loss: 2.291207069158554\n",
      "Epoch 3 loss: 2.2179605066776276\n",
      "Epoch 4 loss: 2.0343845903873445\n",
      "Epoch 5 loss: 1.9125255972146988\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.814309123158455\n",
      "Epoch 2 loss: 1.727859589457512\n",
      "Epoch 3 loss: 1.6253143459558488\n",
      "Epoch 4 loss: 1.5876340180635453\n",
      "Epoch 5 loss: 1.5341704398393632\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.840474370121956\n",
      "Epoch 2 loss: 1.7407340854406357\n",
      "Epoch 3 loss: 1.6699492156505584\n",
      "Epoch 4 loss: 1.6184699714183808\n",
      "Epoch 5 loss: 1.542008323967457\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.833608740568161\n",
      "Epoch 2 loss: 1.7166762799024582\n",
      "Epoch 3 loss: 1.6428762525320053\n",
      "Epoch 4 loss: 1.5700841873884201\n",
      "Epoch 5 loss: 1.5106013774871827\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.8424478381872178\n",
      "Epoch 2 loss: 1.8264641225337983\n",
      "Epoch 3 loss: 1.6974422186613083\n",
      "Epoch 4 loss: 1.627768298983574\n",
      "Epoch 5 loss: 1.6325146764516831\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8312860757112503\n",
      "Epoch 2 loss: 1.7529889643192291\n",
      "Epoch 3 loss: 1.6481748253107071\n",
      "Epoch 4 loss: 1.6098652124404906\n",
      "Epoch 5 loss: 1.5639995515346528\n",
      "\n",
      "\n",
      "Previous Accuracy: 9.5\n",
      "Previous Loss: 0.11436925083398819\n",
      "Current Accuracy: 44.13\n",
      "Current Loss: 0.07113457471132278\n",
      "New state: [0.08       0.07113457]\n",
      "Reward: 34.63\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.13 Current loss:  0.07113457471132278\n",
      "Current mu:  0.08000000000000002\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.2997771501541138\n",
      "Epoch 2 loss: 2.2861466884613035\n",
      "Epoch 3 loss: 2.2075342893600465\n",
      "Epoch 4 loss: 2.0452676683664324\n",
      "Epoch 5 loss: 1.9518475145101548\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3006238996982575\n",
      "Epoch 2 loss: 2.287907975912094\n",
      "Epoch 3 loss: 2.2129463732242582\n",
      "Epoch 4 loss: 2.059782961010933\n",
      "Epoch 5 loss: 1.9575621873140334\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.300316607952118\n",
      "Epoch 2 loss: 2.2850979268550873\n",
      "Epoch 3 loss: 2.193196678161621\n",
      "Epoch 4 loss: 2.0284711122512817\n",
      "Epoch 5 loss: 1.9239398121833802\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.300557804107666\n",
      "Epoch 2 loss: 2.288430690765381\n",
      "Epoch 3 loss: 2.2119272112846375\n",
      "Epoch 4 loss: 2.0532048493623734\n",
      "Epoch 5 loss: 1.9494979113340378\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3003896355628966\n",
      "Epoch 2 loss: 2.2853367388248444\n",
      "Epoch 3 loss: 2.193527352809906\n",
      "Epoch 4 loss: 2.041150289773941\n",
      "Epoch 5 loss: 1.9275836616754531\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8318167239427567\n",
      "Epoch 2 loss: 1.7221404314041138\n",
      "Epoch 3 loss: 1.652681827545166\n",
      "Epoch 4 loss: 1.6070949375629424\n",
      "Epoch 5 loss: 1.5620343804359436\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.882096102833748\n",
      "Epoch 2 loss: 1.75299174785614\n",
      "Epoch 3 loss: 1.6707971394062042\n",
      "Epoch 4 loss: 1.6170249342918397\n",
      "Epoch 5 loss: 1.5397151082754135\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8076050758361817\n",
      "Epoch 2 loss: 1.7107058137655258\n",
      "Epoch 3 loss: 1.6033126652240752\n",
      "Epoch 4 loss: 1.5741607755422593\n",
      "Epoch 5 loss: 1.5262479841709138\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.8671953141689301\n",
      "Epoch 2 loss: 1.7446241438388825\n",
      "Epoch 3 loss: 1.663052898645401\n",
      "Epoch 4 loss: 1.6227472513914107\n",
      "Epoch 5 loss: 1.5676555722951888\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.826401737332344\n",
      "Epoch 2 loss: 1.724110770225525\n",
      "Epoch 3 loss: 1.6439162075519562\n",
      "Epoch 4 loss: 1.5698787927627564\n",
      "Epoch 5 loss: 1.5636534720659256\n",
      "\n",
      "\n",
      "Previous Accuracy: 8.68\n",
      "Previous Loss: 0.11508236080408096\n",
      "Current Accuracy: 45.61\n",
      "Current Loss: 0.06783551722764969\n",
      "New state: [0.09       0.06783552]\n",
      "Reward: 36.93\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  45.61 Current loss:  0.06783551722764969\n",
      "Current mu:  0.09000000000000001\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 34.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 0        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 5        |\n",
      "---------------------------------\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.1\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.303176134824753\n",
      "Epoch 2 loss: 2.297605735063553\n",
      "Epoch 3 loss: 2.286370885372162\n",
      "Epoch 4 loss: 2.220401203632355\n",
      "Epoch 5 loss: 1.9915723383426667\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3036481916904448\n",
      "Epoch 2 loss: 2.298119843006134\n",
      "Epoch 3 loss: 2.2881494581699373\n",
      "Epoch 4 loss: 2.232778126001358\n",
      "Epoch 5 loss: 2.0327555745840074\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3054148256778717\n",
      "Epoch 2 loss: 2.2973942697048186\n",
      "Epoch 3 loss: 2.283947640657425\n",
      "Epoch 4 loss: 2.19671211540699\n",
      "Epoch 5 loss: 1.9698192447423934\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.302775192260742\n",
      "Epoch 2 loss: 2.297512584924698\n",
      "Epoch 3 loss: 2.2870555222034454\n",
      "Epoch 4 loss: 2.2299188554286955\n",
      "Epoch 5 loss: 2.0177832514047624\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.30478013753891\n",
      "Epoch 2 loss: 2.298250991106033\n",
      "Epoch 3 loss: 2.28624541759491\n",
      "Epoch 4 loss: 2.219908195734024\n",
      "Epoch 5 loss: 1.9937475740909576\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8879103302955627\n",
      "Epoch 2 loss: 1.8046617031097412\n",
      "Epoch 3 loss: 1.724261263012886\n",
      "Epoch 4 loss: 1.6380305767059327\n",
      "Epoch 5 loss: 1.5800801157951354\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.9137912482023238\n",
      "Epoch 2 loss: 1.819613292813301\n",
      "Epoch 3 loss: 1.7471585065126418\n",
      "Epoch 4 loss: 1.6740801960229874\n",
      "Epoch 5 loss: 1.6222268491983414\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8632124364376068\n",
      "Epoch 2 loss: 1.7782374292612075\n",
      "Epoch 3 loss: 1.6764201760292052\n",
      "Epoch 4 loss: 1.6148554354906082\n",
      "Epoch 5 loss: 1.5477471023797988\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.9027767807245255\n",
      "Epoch 2 loss: 1.817164161801338\n",
      "Epoch 3 loss: 1.7407857596874237\n",
      "Epoch 4 loss: 1.6765802204608917\n",
      "Epoch 5 loss: 1.613459199666977\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8926714211702347\n",
      "Epoch 2 loss: 1.8074134349823\n",
      "Epoch 3 loss: 1.7434466004371643\n",
      "Epoch 4 loss: 1.648967769742012\n",
      "Epoch 5 loss: 1.616862267255783\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11582213640213013\n",
      "Current Accuracy: 43.21\n",
      "Current Loss: 0.07428443431854248\n",
      "New state: [0.1        0.07428443]\n",
      "Reward: 33.21\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  43.21 Current loss:  0.07428443431854248\n",
      "Current mu:  0.1\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.11\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.302828848361969\n",
      "Epoch 2 loss: 2.298203158378601\n",
      "Epoch 3 loss: 2.282161182165146\n",
      "Epoch 4 loss: 2.1593411594629286\n",
      "Epoch 5 loss: 1.9707361072301866\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3040395736694337\n",
      "Epoch 2 loss: 2.2987295985221863\n",
      "Epoch 3 loss: 2.284314531087875\n",
      "Epoch 4 loss: 2.1687651544809343\n",
      "Epoch 5 loss: 1.9965718299150468\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.303765046596527\n",
      "Epoch 2 loss: 2.2972437143325806\n",
      "Epoch 3 loss: 2.2771051049232485\n",
      "Epoch 4 loss: 2.133734741806984\n",
      "Epoch 5 loss: 1.9451696187257768\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.303840917348862\n",
      "Epoch 2 loss: 2.2987817466259\n",
      "Epoch 3 loss: 2.2836673140525816\n",
      "Epoch 4 loss: 2.173032647371292\n",
      "Epoch 5 loss: 2.0058949053287507\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.303677201271057\n",
      "Epoch 2 loss: 2.2981744408607483\n",
      "Epoch 3 loss: 2.2801699936389923\n",
      "Epoch 4 loss: 2.158422756195068\n",
      "Epoch 5 loss: 1.99428288936615\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8683123409748077\n",
      "Epoch 2 loss: 1.7574383050203324\n",
      "Epoch 3 loss: 1.6893373489379884\n",
      "Epoch 4 loss: 1.6209736436605453\n",
      "Epoch 5 loss: 1.5500101894140244\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.8937219381332397\n",
      "Epoch 2 loss: 1.814869910478592\n",
      "Epoch 3 loss: 1.7284604161977768\n",
      "Epoch 4 loss: 1.6589395076036453\n",
      "Epoch 5 loss: 1.6060119837522506\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8603582978248596\n",
      "Epoch 2 loss: 1.753078007698059\n",
      "Epoch 3 loss: 1.6688994258642196\n",
      "Epoch 4 loss: 1.5779823422431947\n",
      "Epoch 5 loss: 1.556177443265915\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.90302472114563\n",
      "Epoch 2 loss: 1.7988882571458817\n",
      "Epoch 3 loss: 1.6999131172895432\n",
      "Epoch 4 loss: 1.632744362950325\n",
      "Epoch 5 loss: 1.5878726363182067\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8734969794750214\n",
      "Epoch 2 loss: 1.779628297686577\n",
      "Epoch 3 loss: 1.7040404498577117\n",
      "Epoch 4 loss: 1.63355633020401\n",
      "Epoch 5 loss: 1.6320261865854264\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "   Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>  \n",
      " Traceback (most recent call last):\n",
      "   File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "      ^self._shutdown_workers()^\n",
      "^^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "^^    if w.is_alive():^\n",
      "^^  ^ ^ ^  ^ ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^^ ^ ^ ^ \n",
      " AssertionError:  can only test a child process \n",
      "    Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "^^    ^self._shutdown_workers()^\n",
      "^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "^^    ^^if w.is_alive():^\n",
      "^ ^ ^  ^  ^ ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "\n",
      "    AssertionError: assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
      "\n",
      "  Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0> \n",
      " Traceback (most recent call last):\n",
      "   File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "       self._shutdown_workers() \n",
      " ^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "^^    ^if w.is_alive():^\n",
      "^ ^ ^ ^ ^ ^^  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^ ^ ^  ^ ^\n",
      " AssertionError :  can only test a child process \n",
      " ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Accuracy: 8.96\n",
      "Previous Loss: 0.11483438313007355\n",
      "Current Accuracy: 43.32\n",
      "Current Loss: 0.07230949401855469\n",
      "New state: [0.11       0.07230949]\n",
      "Reward: 34.36\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0><function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "        if w.is_alive():if w.is_alive():\n",
      "\n",
      "             ^ ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
      "             ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^: ^can only test a child process^\n",
      "^^\n",
      "Exception ignored in: AssertionError: <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>can only test a child process\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "Exception ignored in:     self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "Traceback (most recent call last):\n",
      "      File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "if w.is_alive():\n",
      "      self._shutdown_workers() \n",
      "   File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "       if w.is_alive():^^\n",
      "^ ^ ^^ ^ ^ ^^ ^ ^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      " ^  ^  ^ ^  ^  ^ ^^^^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "^^  ^ ^  ^ ^  ^ ^^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^: ^can only test a child process^\n",
      "^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current accuracy:  43.32 Current loss:  0.07230949401855469\n",
      "Current mu:  0.11\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>^\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "self._shutdown_workers() \n",
      "   File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "         if w.is_alive(): \n",
      "       ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^ ^ ^ ^  ^ ^  ^  ^^^^^^^\n",
      "^AssertionError^: ^^can only test a child process^\n",
      "^^^^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "^^    ^self._shutdown_workers()^\n",
      "^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "^^    ^^if w.is_alive():\n",
      "^ ^ ^ ^^ \n",
      " AssertionError :  can only test a child process^\n",
      "^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x77c54f5bb1a0>^\n",
      "^Traceback (most recent call last):\n",
      "^  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "^^    ^self._shutdown_workers()^\n",
      "\n",
      "  File \"/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "              ^^ ^ ^^ ^ ^^^^^^^^^^^\n",
      "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^ ^ ^ ^ ^  ^  ^ ^  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^^: ^^can only test a child process^\n",
      "^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3023500084877013\n",
      "Epoch 2 loss: 2.2931070983409882\n",
      "Epoch 3 loss: 2.2476810693740843\n",
      "Epoch 4 loss: 2.0661223620176314\n",
      "Epoch 5 loss: 1.9375873804092407\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3024299681186675\n",
      "Epoch 2 loss: 2.29418198466301\n",
      "Epoch 3 loss: 2.2571068465709687\n",
      "Epoch 4 loss: 2.09761458337307\n",
      "Epoch 5 loss: 1.9670650213956833\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3019712626934052\n",
      "Epoch 2 loss: 2.293469172716141\n",
      "Epoch 3 loss: 2.253569686412811\n",
      "Epoch 4 loss: 2.0696848213672636\n",
      "Epoch 5 loss: 1.9217627942562103\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.3028565764427187\n",
      "Epoch 2 loss: 2.294799154996872\n",
      "Epoch 3 loss: 2.256499099731445\n",
      "Epoch 4 loss: 2.0983267426490784\n",
      "Epoch 5 loss: 1.9576943397521973\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3026231586933137\n",
      "Epoch 2 loss: 2.293158155679703\n",
      "Epoch 3 loss: 2.2445386409759522\n",
      "Epoch 4 loss: 2.0754198700189592\n",
      "Epoch 5 loss: 1.9478893637657166\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8353847444057465\n",
      "Epoch 2 loss: 1.7502587199211121\n",
      "Epoch 3 loss: 1.6359524458646775\n",
      "Epoch 4 loss: 1.5718743801116943\n",
      "Epoch 5 loss: 1.537941187620163\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.854031854867935\n",
      "Epoch 2 loss: 1.7574086964130402\n",
      "Epoch 3 loss: 1.6862237453460693\n",
      "Epoch 4 loss: 1.6190247654914856\n",
      "Epoch 5 loss: 1.558136761188507\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8009208291769028\n",
      "Epoch 2 loss: 1.7243866205215455\n",
      "Epoch 3 loss: 1.621959075331688\n",
      "Epoch 4 loss: 1.5740640491247178\n",
      "Epoch 5 loss: 1.5141066402196883\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.8556489497423172\n",
      "Epoch 2 loss: 1.7851848423480987\n",
      "Epoch 3 loss: 1.676760870218277\n",
      "Epoch 4 loss: 1.6270827680826188\n",
      "Epoch 5 loss: 1.5550265699625014\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8227797210216523\n",
      "Epoch 2 loss: 1.7322908133268355\n",
      "Epoch 3 loss: 1.6548283129930497\n",
      "Epoch 4 loss: 1.5815230637788773\n",
      "Epoch 5 loss: 1.5770477086305619\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11528100818395615\n",
      "Current Accuracy: 44.65\n",
      "Current Loss: 0.0735572949051857\n",
      "New state: [0.1        0.07355729]\n",
      "Reward: 34.65\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.65 Current loss:  0.0735572949051857\n",
      "Current mu:  0.1\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3028834223747254\n",
      "Epoch 2 loss: 2.294809466600418\n",
      "Epoch 3 loss: 2.2648623406887056\n",
      "Epoch 4 loss: 2.0711841851472856\n",
      "Epoch 5 loss: 1.9331682801246644\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3023676574230194\n",
      "Epoch 2 loss: 2.294374221563339\n",
      "Epoch 3 loss: 2.266664135456085\n",
      "Epoch 4 loss: 2.0943742036819457\n",
      "Epoch 5 loss: 1.9376563400030136\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3020416676998137\n",
      "Epoch 2 loss: 2.293421041965485\n",
      "Epoch 3 loss: 2.260545253753662\n",
      "Epoch 4 loss: 2.0720605850219727\n",
      "Epoch 5 loss: 1.8903818875551224\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.3024400651454924\n",
      "Epoch 2 loss: 2.2949845254421235\n",
      "Epoch 3 loss: 2.270280957221985\n",
      "Epoch 4 loss: 2.1056262493133544\n",
      "Epoch 5 loss: 1.9302349090576172\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3024088382720946\n",
      "Epoch 2 loss: 2.2943249940872192\n",
      "Epoch 3 loss: 2.267579847574234\n",
      "Epoch 4 loss: 2.0987585365772246\n",
      "Epoch 5 loss: 1.9356705486774444\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.783819818496704\n",
      "Epoch 2 loss: 1.6977100640535354\n",
      "Epoch 3 loss: 1.6197242259979248\n",
      "Epoch 4 loss: 1.5695680558681488\n",
      "Epoch 5 loss: 1.496818146109581\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.812063878774643\n",
      "Epoch 2 loss: 1.6997154653072357\n",
      "Epoch 3 loss: 1.6352709621191024\n",
      "Epoch 4 loss: 1.5970690190792083\n",
      "Epoch 5 loss: 1.5721332490444184\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.7500259339809419\n",
      "Epoch 2 loss: 1.6763550907373428\n",
      "Epoch 3 loss: 1.5958742588758468\n",
      "Epoch 4 loss: 1.5466318815946578\n",
      "Epoch 5 loss: 1.4797349214553832\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.8222218662500382\n",
      "Epoch 2 loss: 1.7224295914173127\n",
      "Epoch 3 loss: 1.6835981816053391\n",
      "Epoch 4 loss: 1.5967017829418182\n",
      "Epoch 5 loss: 1.6088331758975982\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.7953439772129058\n",
      "Epoch 2 loss: 1.7109546303749084\n",
      "Epoch 3 loss: 1.6248361706733703\n",
      "Epoch 4 loss: 1.5812845677137375\n",
      "Epoch 5 loss: 1.514052864909172\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.06\n",
      "Previous Loss: 0.11516093462705612\n",
      "Current Accuracy: 44.96\n",
      "Current Loss: 0.06880633533000946\n",
      "New state: [0.09       0.06880634]\n",
      "Reward: 34.9\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.96 Current loss:  0.06880633533000946\n",
      "Current mu:  0.09000000000000001\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.08000000000000002\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.301877611875534\n",
      "Epoch 2 loss: 2.2924741089344023\n",
      "Epoch 3 loss: 2.246439057588577\n",
      "Epoch 4 loss: 2.1122610211372375\n",
      "Epoch 5 loss: 1.9716405361890792\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3017467319965363\n",
      "Epoch 2 loss: 2.29450044631958\n",
      "Epoch 3 loss: 2.2577116787433624\n",
      "Epoch 4 loss: 2.1308791816234587\n",
      "Epoch 5 loss: 2.0021508038043976\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.302443206310272\n",
      "Epoch 2 loss: 2.2930053234100343\n",
      "Epoch 3 loss: 2.2511727154254912\n",
      "Epoch 4 loss: 2.103797173500061\n",
      "Epoch 5 loss: 1.940328946709633\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.302195107936859\n",
      "Epoch 2 loss: 2.29292408823967\n",
      "Epoch 3 loss: 2.253114455938339\n",
      "Epoch 4 loss: 2.1289167612791062\n",
      "Epoch 5 loss: 1.986705407500267\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3014398992061613\n",
      "Epoch 2 loss: 2.291487467288971\n",
      "Epoch 3 loss: 2.246626818180084\n",
      "Epoch 4 loss: 2.116002181172371\n",
      "Epoch 5 loss: 1.995573428273201\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.9065688461065293\n",
      "Epoch 2 loss: 1.798386988043785\n",
      "Epoch 3 loss: 1.7132286518812179\n",
      "Epoch 4 loss: 1.624464550614357\n",
      "Epoch 5 loss: 1.5852879345417024\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.914388480782509\n",
      "Epoch 2 loss: 1.8165255695581437\n",
      "Epoch 3 loss: 1.726570376753807\n",
      "Epoch 4 loss: 1.6527588427066804\n",
      "Epoch 5 loss: 1.5999749571084976\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8569576859474182\n",
      "Epoch 2 loss: 1.7865237206220628\n",
      "Epoch 3 loss: 1.699697408080101\n",
      "Epoch 4 loss: 1.6134153246879577\n",
      "Epoch 5 loss: 1.5370776623487472\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.9011525332927703\n",
      "Epoch 2 loss: 1.8406850308179856\n",
      "Epoch 3 loss: 1.7709760904312133\n",
      "Epoch 4 loss: 1.6728168070316314\n",
      "Epoch 5 loss: 1.629627165198326\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8879011213779449\n",
      "Epoch 2 loss: 1.793315950036049\n",
      "Epoch 3 loss: 1.721497866511345\n",
      "Epoch 4 loss: 1.6678990334272386\n",
      "Epoch 5 loss: 1.6278969585895537\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11571192741394043\n",
      "Current Accuracy: 42.51\n",
      "Current Loss: 0.06578534841537476\n",
      "New state: [0.08       0.06578535]\n",
      "Reward: 32.51\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  42.51 Current loss:  0.06578534841537476\n",
      "Current mu:  0.08000000000000002\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1             |\n",
      "|    ep_rew_mean          | 34.4          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 0             |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 232           |\n",
      "|    total_timesteps      | 10            |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3315678e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.693        |\n",
      "|    explained_variance   | -0.00202      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 604           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000445     |\n",
      "|    value_loss           | 1.21e+03      |\n",
      "-------------------------------------------\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3014279901981354\n",
      "Epoch 2 loss: 2.291928839683533\n",
      "Epoch 3 loss: 2.243604081869125\n",
      "Epoch 4 loss: 2.0680826872587206\n",
      "Epoch 5 loss: 1.9700310677289963\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3020339131355287\n",
      "Epoch 2 loss: 2.2922024607658384\n",
      "Epoch 3 loss: 2.245051610469818\n",
      "Epoch 4 loss: 2.087152582406998\n",
      "Epoch 5 loss: 1.9832109928131103\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.302197790145874\n",
      "Epoch 2 loss: 2.290940946340561\n",
      "Epoch 3 loss: 2.242830294370651\n",
      "Epoch 4 loss: 2.0639785557985304\n",
      "Epoch 5 loss: 1.9352970659732818\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.302572172880173\n",
      "Epoch 2 loss: 2.2927972435951234\n",
      "Epoch 3 loss: 2.2491255164146424\n",
      "Epoch 4 loss: 2.089835932850838\n",
      "Epoch 5 loss: 1.9897984743118287\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.302448844909668\n",
      "Epoch 2 loss: 2.2918334662914277\n",
      "Epoch 3 loss: 2.2369715332984925\n",
      "Epoch 4 loss: 2.081163930892944\n",
      "Epoch 5 loss: 1.9634295374155044\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.8686012268066405\n",
      "Epoch 2 loss: 1.7900651514530181\n",
      "Epoch 3 loss: 1.7132906824350358\n",
      "Epoch 4 loss: 1.6249926060438156\n",
      "Epoch 5 loss: 1.584210041165352\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.908599066734314\n",
      "Epoch 2 loss: 1.8130051255226136\n",
      "Epoch 3 loss: 1.7316849559545517\n",
      "Epoch 4 loss: 1.6587117969989777\n",
      "Epoch 5 loss: 1.6168786257505416\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8390837281942367\n",
      "Epoch 2 loss: 1.755455780029297\n",
      "Epoch 3 loss: 1.6548956334590912\n",
      "Epoch 4 loss: 1.6045681923627853\n",
      "Epoch 5 loss: 1.5518542170524596\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.915174213051796\n",
      "Epoch 2 loss: 1.8152173817157746\n",
      "Epoch 3 loss: 1.7271489828824997\n",
      "Epoch 4 loss: 1.6828878283500672\n",
      "Epoch 5 loss: 1.6169508278369904\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.8666766673326491\n",
      "Epoch 2 loss: 1.7889544427394868\n",
      "Epoch 3 loss: 1.6947326689958573\n",
      "Epoch 4 loss: 1.6434980809688569\n",
      "Epoch 5 loss: 1.5810664623975754\n",
      "\n",
      "\n",
      "Previous Accuracy: 7.42\n",
      "Previous Loss: 0.11608128994703293\n",
      "Current Accuracy: 43.42\n",
      "Current Loss: 0.0749548077583313\n",
      "New state: [0.09       0.07495481]\n",
      "Reward: 36.0\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  43.42 Current loss:  0.0749548077583313\n",
      "Current mu:  0.09000000000000001\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.1\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.300305998325348\n",
      "Epoch 2 loss: 2.2841604709625245\n",
      "Epoch 3 loss: 2.174880576133728\n",
      "Epoch 4 loss: 1.9835156619548797\n",
      "Epoch 5 loss: 1.872366714477539\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3006843507289885\n",
      "Epoch 2 loss: 2.285028004646301\n",
      "Epoch 3 loss: 2.1864147067070006\n",
      "Epoch 4 loss: 2.011281096935272\n",
      "Epoch 5 loss: 1.8935599446296691\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3006747126579286\n",
      "Epoch 2 loss: 2.2823840498924257\n",
      "Epoch 3 loss: 2.156327748298645\n",
      "Epoch 4 loss: 1.9532725244760514\n",
      "Epoch 5 loss: 1.8474524974823\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.3008069694042206\n",
      "Epoch 2 loss: 2.285679465532303\n",
      "Epoch 3 loss: 2.186836767196655\n",
      "Epoch 4 loss: 1.9909807562828064\n",
      "Epoch 5 loss: 1.8904292970895766\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3005194306373595\n",
      "Epoch 2 loss: 2.282977569103241\n",
      "Epoch 3 loss: 2.160968065261841\n",
      "Epoch 4 loss: 1.9758406370878219\n",
      "Epoch 5 loss: 1.8798853218555451\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.7687387436628341\n",
      "Epoch 2 loss: 1.7062550485134125\n",
      "Epoch 3 loss: 1.626449105143547\n",
      "Epoch 4 loss: 1.590811362862587\n",
      "Epoch 5 loss: 1.53104487657547\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.7976738154888152\n",
      "Epoch 2 loss: 1.7098598241806031\n",
      "Epoch 3 loss: 1.7076634287834167\n",
      "Epoch 4 loss: 1.59614078104496\n",
      "Epoch 5 loss: 1.550967174768448\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.7367193281650544\n",
      "Epoch 2 loss: 1.6670165956020355\n",
      "Epoch 3 loss: 1.6093616515398026\n",
      "Epoch 4 loss: 1.5516404509544373\n",
      "Epoch 5 loss: 1.4969282478094101\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.7877228647470473\n",
      "Epoch 2 loss: 1.715450745820999\n",
      "Epoch 3 loss: 1.6611285388469696\n",
      "Epoch 4 loss: 1.598296508193016\n",
      "Epoch 5 loss: 1.5733389258384705\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.7571861058473588\n",
      "Epoch 2 loss: 1.6893853336572646\n",
      "Epoch 3 loss: 1.6274425029754638\n",
      "Epoch 4 loss: 1.6207643389701842\n",
      "Epoch 5 loss: 1.5544934898614884\n",
      "\n",
      "\n",
      "Previous Accuracy: 9.32\n",
      "Previous Loss: 0.11543158441781998\n",
      "Current Accuracy: 44.83\n",
      "Current Loss: 0.06786751002073288\n",
      "New state: [0.1        0.06786751]\n",
      "Reward: 35.51\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.83 Current loss:  0.06786751002073288\n",
      "Current mu:  0.1\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.301484501361847\n",
      "Epoch 2 loss: 2.290874344110489\n",
      "Epoch 3 loss: 2.230092483758926\n",
      "Epoch 4 loss: 2.0986498653888703\n",
      "Epoch 5 loss: 1.9802009791135788\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.302291464805603\n",
      "Epoch 2 loss: 2.29253403544426\n",
      "Epoch 3 loss: 2.2458536624908447\n",
      "Epoch 4 loss: 2.117862045764923\n",
      "Epoch 5 loss: 2.0085842221975327\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3024231612682344\n",
      "Epoch 2 loss: 2.289798855781555\n",
      "Epoch 3 loss: 2.22667281627655\n",
      "Epoch 4 loss: 2.0879413783550262\n",
      "Epoch 5 loss: 1.9591755777597428\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.302293622493744\n",
      "Epoch 2 loss: 2.2925072729587557\n",
      "Epoch 3 loss: 2.2405973613262176\n",
      "Epoch 4 loss: 2.1158746272325515\n",
      "Epoch 5 loss: 2.0007643699645996\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.302850979566574\n",
      "Epoch 2 loss: 2.2909684062004088\n",
      "Epoch 3 loss: 2.234328085184097\n",
      "Epoch 4 loss: 2.108803254365921\n",
      "Epoch 5 loss: 1.9907356530427933\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.9003551930189133\n",
      "Epoch 2 loss: 1.8173580467700958\n",
      "Epoch 3 loss: 1.6906355440616607\n",
      "Epoch 4 loss: 1.6208760559558868\n",
      "Epoch 5 loss: 1.5955816358327866\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.905519238114357\n",
      "Epoch 2 loss: 1.8484546452760697\n",
      "Epoch 3 loss: 1.728575873374939\n",
      "Epoch 4 loss: 1.6498640954494477\n",
      "Epoch 5 loss: 1.6051019221544265\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.8514552861452103\n",
      "Epoch 2 loss: 1.734714087843895\n",
      "Epoch 3 loss: 1.6726286113262177\n",
      "Epoch 4 loss: 1.5985058039426803\n",
      "Epoch 5 loss: 1.5455917418003082\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.922502601146698\n",
      "Epoch 2 loss: 1.8089833080768585\n",
      "Epoch 3 loss: 1.7337776303291321\n",
      "Epoch 4 loss: 1.6711488604545592\n",
      "Epoch 5 loss: 1.6143080651760102\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.9048190504312514\n",
      "Epoch 2 loss: 1.796618428826332\n",
      "Epoch 3 loss: 1.6937725335359572\n",
      "Epoch 4 loss: 1.616060835123062\n",
      "Epoch 5 loss: 1.5714440524578095\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11662039905786514\n",
      "Current Accuracy: 44.01\n",
      "Current Loss: 0.06821523606777191\n",
      "New state: [0.09       0.06821524]\n",
      "Reward: 34.01\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  44.01 Current loss:  0.06821523606777191\n",
      "Current mu:  0.09000000000000001\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 1\n",
      "Increasing mu to 0.1\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.303532087802887\n",
      "Epoch 2 loss: 2.2960215508937836\n",
      "Epoch 3 loss: 2.2742474377155304\n",
      "Epoch 4 loss: 2.1657788813114167\n",
      "Epoch 5 loss: 2.0316630601882935\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.302863967418671\n",
      "Epoch 2 loss: 2.2975384891033173\n",
      "Epoch 3 loss: 2.279989278316498\n",
      "Epoch 4 loss: 2.1928238034248353\n",
      "Epoch 5 loss: 2.0708034366369246\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.3034264504909516\n",
      "Epoch 2 loss: 2.2953680038452147\n",
      "Epoch 3 loss: 2.2664836168289186\n",
      "Epoch 4 loss: 2.151189160346985\n",
      "Epoch 5 loss: 2.0390058726072313\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.303476148843765\n",
      "Epoch 2 loss: 2.2975883185863495\n",
      "Epoch 3 loss: 2.280804896354675\n",
      "Epoch 4 loss: 2.1912980377674103\n",
      "Epoch 5 loss: 2.068934527039528\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.303837949037552\n",
      "Epoch 2 loss: 2.2961429595947265\n",
      "Epoch 3 loss: 2.2716177463531495\n",
      "Epoch 4 loss: 2.1538123726844787\n",
      "Epoch 5 loss: 2.040723133087158\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.9271578371524811\n",
      "Epoch 2 loss: 1.819853138923645\n",
      "Epoch 3 loss: 1.7400253802537917\n",
      "Epoch 4 loss: 1.6532599747180938\n",
      "Epoch 5 loss: 1.5878599613904953\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.9639818996191025\n",
      "Epoch 2 loss: 1.8466927111148834\n",
      "Epoch 3 loss: 1.7430775433778762\n",
      "Epoch 4 loss: 1.680161678791046\n",
      "Epoch 5 loss: 1.6285216480493545\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.9075519621372223\n",
      "Epoch 2 loss: 1.7847912669181825\n",
      "Epoch 3 loss: 1.6823867052793502\n",
      "Epoch 4 loss: 1.6004055708646774\n",
      "Epoch 5 loss: 1.563587275147438\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.9400096386671066\n",
      "Epoch 2 loss: 1.8340009540319442\n",
      "Epoch 3 loss: 1.7388036638498305\n",
      "Epoch 4 loss: 1.6610704898834228\n",
      "Epoch 5 loss: 1.5981256425380708\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.9092519223690032\n",
      "Epoch 2 loss: 1.806008219718933\n",
      "Epoch 3 loss: 1.7284248024225235\n",
      "Epoch 4 loss: 1.6294386744499207\n",
      "Epoch 5 loss: 1.6031767934560777\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11539508402347565\n",
      "Current Accuracy: 42.45\n",
      "Current Loss: 0.07149159163236618\n",
      "New state: [0.1        0.07149159]\n",
      "Reward: 32.45\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  42.45 Current loss:  0.07149159163236618\n",
      "Current mu:  0.1\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.09000000000000001\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.301446330547333\n",
      "Epoch 2 loss: 2.2826606392860413\n",
      "Epoch 3 loss: 2.1628471195697783\n",
      "Epoch 4 loss: 1.9765509247779847\n",
      "Epoch 5 loss: 1.8506418019533157\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3016708195209503\n",
      "Epoch 2 loss: 2.2839291214942934\n",
      "Epoch 3 loss: 2.180088019371033\n",
      "Epoch 4 loss: 2.001330080628395\n",
      "Epoch 5 loss: 1.8774777591228484\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 2.300866049528122\n",
      "Epoch 2 loss: 2.280877095460892\n",
      "Epoch 3 loss: 2.1590120911598207\n",
      "Epoch 4 loss: 1.946719118952751\n",
      "Epoch 5 loss: 1.828799334168434\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 2.301716905832291\n",
      "Epoch 2 loss: 2.2828885436058046\n",
      "Epoch 3 loss: 2.167467510700226\n",
      "Epoch 4 loss: 1.9773740082979203\n",
      "Epoch 5 loss: 1.8766247004270553\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 2.3012010991573333\n",
      "Epoch 2 loss: 2.281085193157196\n",
      "Epoch 3 loss: 2.15444501042366\n",
      "Epoch 4 loss: 1.9739534944295882\n",
      "Epoch 5 loss: 1.8516425669193268\n",
      "\n",
      "\n",
      "---- Global Round 2: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 1.7549010932445526\n",
      "Epoch 2 loss: 1.6767569750547409\n",
      "Epoch 3 loss: 1.607606115937233\n",
      "Epoch 4 loss: 1.5902891248464583\n",
      "Epoch 5 loss: 1.518902948498726\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 1.7815619617700578\n",
      "Epoch 2 loss: 1.7175009429454804\n",
      "Epoch 3 loss: 1.6534507542848587\n",
      "Epoch 4 loss: 1.5873402237892151\n",
      "Epoch 5 loss: 1.5284214287996292\n",
      "\n",
      "\n",
      "Client 3 starts training...\n",
      "Epoch 1 loss: 1.739545139670372\n",
      "Epoch 2 loss: 1.6357839316129685\n",
      "Epoch 3 loss: 1.6119486093521118\n",
      "Epoch 4 loss: 1.5209716022014619\n",
      "Epoch 5 loss: 1.4952223181724549\n",
      "\n",
      "\n",
      "Client 4 starts training...\n",
      "Epoch 1 loss: 1.7757985979318618\n",
      "Epoch 2 loss: 1.7111707478761673\n",
      "Epoch 3 loss: 1.6202277719974518\n",
      "Epoch 4 loss: 1.585152330994606\n",
      "Epoch 5 loss: 1.512795904278755\n",
      "\n",
      "\n",
      "Client 5 starts training...\n",
      "Epoch 1 loss: 1.7433952242136002\n",
      "Epoch 2 loss: 1.6677356362342834\n",
      "Epoch 3 loss: 1.6083020865917206\n",
      "Epoch 4 loss: 1.596964505314827\n",
      "Epoch 5 loss: 1.520471179485321\n",
      "\n",
      "\n",
      "Previous Accuracy: 10.0\n",
      "Previous Loss: 0.11671008914709091\n",
      "Current Accuracy: 45.73\n",
      "Current Loss: 0.07067326456308365\n",
      "New state: [0.09       0.07067326]\n",
      "Reward: 35.73\n",
      "\n",
      "'--------------End Federated Learning--------------'\n",
      "\n",
      "\n",
      "'=======================================Restarting Environment======================================='\n",
      "\n",
      "Current accuracy:  45.73 Current loss:  0.07067326456308365\n",
      "Current mu:  0.09000000000000001\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 34.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 15           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.053116e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.00358      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 571          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.000105     |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "\n",
      "'=====================Stepping====================='\n",
      "\n",
      "Action take: 0\n",
      "Decreasing mu to 0.08000000000000002\n",
      "\n",
      "'-------------Start Federated Learning-------------'\n",
      "\n",
      "---- Global Round 1: ----\n",
      "Client 1 starts training...\n",
      "Epoch 1 loss: 2.3009015917778015\n",
      "Epoch 2 loss: 2.286653923988342\n",
      "Epoch 3 loss: 2.196785259246826\n",
      "Epoch 4 loss: 2.035548302531242\n",
      "Epoch 5 loss: 1.914041817188263\n",
      "\n",
      "\n",
      "Client 2 starts training...\n",
      "Epoch 1 loss: 2.3014321267604827\n",
      "Epoch 2 loss: 2.2900279760360718\n",
      "Epoch 3 loss: 2.220221096277237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m modelRL \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodelRL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[50], line 51\u001b[0m, in \u001b[0;36mFedProxTuningEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Run one global epoch of federated learning with the current mu\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(f'Running global epoch {self.current_round + 1} with mu = {self.current_mu}')\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Federated Learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m-^50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m global_model \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msys_heter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m current_metrics \u001b[38;5;241m=\u001b[39m evaluate(global_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Update the state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 106\u001b[0m, in \u001b[0;36mfederated_learning\u001b[0;34m(model, mu, client_datasets, testloader, optimizer, loss_function, global_epochs, local_epochs, algorithm, sys_heter)\u001b[0m\n\u001b[1;32m    103\u001b[0m state_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_id, client_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(client_dataloaders):\n\u001b[0;32m--> 106\u001b[0m     client_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mclient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_model_lists\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_optimizer_lists\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43msys_heter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     state_dicts\u001b[38;5;241m.\u001b[39mappend(client_state_dict)\n\u001b[1;32m    109\u001b[0m global_model \u001b[38;5;241m=\u001b[39m server_aggregate(global_model, state_dicts)\n",
      "Cell \u001b[0;32mIn[49], line 42\u001b[0m, in \u001b[0;36mclient_update\u001b[0;34m(received_model, train_data, local_optimizer, loss_f, epoch, client_id, mu, algorithm, sys_heter)\u001b[0m\n\u001b[1;32m     40\u001b[0m local_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m feature, label \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mto(myGPU), label\u001b[38;5;241m.\u001b[39mto(myGPU)\n\u001b[0;32m---> 42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m local_loss \u001b[38;5;241m=\u001b[39m loss_f(outputs, label)\n\u001b[1;32m     44\u001b[0m loss_prox \u001b[38;5;241m=\u001b[39m (mu \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m difference_models_norm_2(local_model, initial_model) \u001b[38;5;66;03m# perform model updates penalization using proximal term\u001b[39;00m\n",
      "File \u001b[0;32m/winlnx/share/Sleepb411/MySpace/Winter2024/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "fl_model = CNN().to(myGPU)\n",
    "# old_global_model = None\n",
    "optimizer = optim.SGD(fl_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# Instantiate the environment\n",
    "env = FedProxTuningEnv(fl_model, client_datasets, testloader, optimizer, loss_function)\n",
    "\n",
    "# Instantiate the agent\n",
    "modelRL = PPO(\"MlpPolicy\", env, verbose=1, n_steps=5)\n",
    "\n",
    "# Train the agent\n",
    "modelRL.learn(total_timesteps=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
